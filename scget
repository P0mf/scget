#!/usr/bin/env python2

import sys
import os
import urllib2
import time

try:
    import requests
except:
    print "You seem to be missing the requests module, please install it before running scget."
    sys.exit()
try:    
    from BeautifulSoup import BeautifulSoup
except:
    print "You seem to be missing the bs4 module, please install it before running scget."
    sys.exit()


args = sys.argv
headers = {
    'User-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:29.0)'
    ' Gecko/20100101 Firefox/29.0'
}
page = 'http://chan.sankakucomplex.com/'
working_dir = ''
quality = {
    'P' : 'order%3Apopular',
    'Q' : 'order%3Aquality',
    'M' : 'order%3Ampixel',
    'R' : 'order%3Arandom',
    'N' : ''
    }

def show_help():
    print 'Usage: scget [MODE] [tag1,tag2,...] [PATH]'
    print 'Available modes:'
    print '\tP\tDownloads images ordered by popularity.'
    print '\tQ\tDownloads images ordered by \"quality\".'
    print '\tM\tDownloads images ordered by resolution.'
    print '\tP\tDownloads images ordered by date added.'
    print '\tR\tDownloads images in a random order.'
    print 'Tags:'
    print '\tDownloads only results matching the tags you enter, separated by commas, no spaces.'
    print '\tANY\tDownloads all the images according to selected mode.'
    print 'Path:'
    print '\tSpecify a path where downloaded images will be saved to.'
    print 'Usage examples:'
    print '\tscget P K-on! ~/pictures'
    print '\tDownloads K-On! tagged pictures ordered by popularity to ~/pictures/K-On!/'
    print '\nReport issues: https://github.com/P0mf/scget/issues'
    print 'Repository: https://github.com/P0mf/scget'

def get_tags():
    if (args[2] == 'ANY'):
        to_return = ''
    else:
        tags = args[2].split(',')
        to_return = ''
        for s in tags:
            to_return += s + '+'
    
    return '?tags=' + to_return
    
def get_quality():
    order = ''
    try :
        order = quality[args[1]]
    except KeyError:
        pass
    return order

# opens a request and returns the page if there is no errors otherwise try again
def request(url, headers):
    while True:
        r = requests.get(url, headers=headers)
        if r.status_code == 200:
            return r.text;
        else:
            print 'HTTP ERROR -> ', r.status_code
            time.sleep(10)
    
# creates the dumping directory and returns its name    
def create_working_dir():
    tags = args[2].split(',')
    dir_name = ''
    for s in tags:
        if ":" in s:
            s = s.split(':')
            dir_name += s[1] + '-'
        else:
            dir_name += s + '-'
    work_dir_abs = args[3] + '/' + dir_name[:-1]
    if not os.path.exists(work_dir_abs):
        os.makedirs(work_dir_abs)
    return work_dir_abs

# get the total number of images for the tag combination
def get_total(link):
    total_images = ""
    soup = BeautifulSoup(request(link, headers))
    for a in soup.findAll("span"):
        if a.get("title") and "Post Count:" in a.get("title"):
            total_images = a.get("title").split(" ")[2]

    return total_images

# get all images(the image page) in a page and returns links to each one     
def get_page_links(link):
    page_links = []
    soup = BeautifulSoup(request(link, headers))
    for a in soup.findAll('a'):
        if(a.get('href').find('/post/show/') != -1):
            page_links.append(page + a.get('href'))
    return page_links

# get the page with its number
def get_page(page_number):
    return page + get_tags() + get_quality() + '&page=' + str(page_number)

# gets the image out of a image page
def get_image(image_page):
    print image_page
    soup = BeautifulSoup(request(image_page, headers))
    try:
        image = soup.find(id="highres").get('href')
    except AttributeError:
        image = 'NOTHING'
    return image
    
# download the image from the url    
def download_image(image, refer):
    image_id = image.rsplit('?')[1]
    format = image.split('.')[3].split('?')[0]
    good = image_id + '.' + format
    
    if not os.path.isfile(working_dir + '/' + good):
        req = urllib2.Request('http:' + image)
        req.add_header('Referer', refer)
        req.add_header('Cookie', '__cfduid=d779ff63e44a27415f'
            '6f993ca8ae7342f1399560942517')
        req.add_header('User-Agent', 'Mozilla/5.0 (X11; Ubuntu; Linux' 
                        +'x86_64; rv:29.0) Gecko/20100101 Firefox/29.0')
        response = urllib2.urlopen(req)
        
        output = open(working_dir + good, 'wb')
        output.write(response.read())
        output.close()
        return good
        
    return False
    
if len(args) == 4:
    downloaded = 0
    working_dir = create_working_dir()
    counter = 1
    total = get_total(get_page(1))
    try:
        while True:
            link = get_page(counter)
            images = get_page_links(link)
            if not images:
                raise KeyboardInterrupt
            for i in xrange(len(images)):
                image = get_image(images[i])
                if image != 'NOTHING':
                    img_name = download_image(image, images[i])
                    if img_name:
                        downloaded = downloaded + 1
                        print img_name, 'Downloaded! - ', downloaded, '/', total
            counter += 1
    except KeyboardInterrupt:
        print '\nTotal Image Downloaded: ', downloaded, '/', total
        print 'Enjoy!'
elif len(args) == 2 and args[1] == "--help":
    show_help()
else:
    print 'path doesn\'t exist or wrong number of arguments'
    print 'scget [P, Q, R, M, N] [tag_1, tag_2, ... OR ANY ] [path]'
    print 'scget --help for in-depth information of arguments'
